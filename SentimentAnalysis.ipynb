{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOBQRV7M/NHD8RlulCXeWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saloniasrani/sentimentanalysis/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing CSV "
      ],
      "metadata": {
        "id": "NaDxbzZ4c4I9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdF7bgalAJIu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Gm8NkVb6DdSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing dataset into pandas dataframe"
      ],
      "metadata": {
        "id": "1azE__nwc-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_document = pd.read_csv(\"/content/drive/MyDrive/SentimentAnalysis/movie.csv\")"
      ],
      "metadata": {
        "id": "CKPMpw_LeVZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading important libraries"
      ],
      "metadata": {
        "id": "K-4ACBJEdH0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tecKzU_0EnFQ",
        "outputId": "86b4d8eb-8095-4539-9621-1c6b03c72200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making an array containing tuples of reviews and their corresponding sentiment/category"
      ],
      "metadata": {
        "id": "quL5hE8WdaK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [(train_document.iloc[i,0],train_document.iloc[i,1]) for i in range(train_document.shape[0])]\n",
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN4vIvORhdHE",
        "outputId": "b0e81342-df07-4a73-8a6c-6d8651d0f9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.',\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "doc = [(word_tokenize(word),category) for word,category in documents]"
      ],
      "metadata": {
        "id": "82qvPJtylyUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc[:5]"
      ],
      "metadata": {
        "id": "xqHPlQUisMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "def get_simple_pos(tag):\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "RbrHa4wSbp3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of stopwords"
      ],
      "metadata": {
        "id": "_pkh4-dndRMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "stops = stopwords.words('english')\n",
        "punctuations = list(string.punctuation)\n",
        "stops += punctuations\n",
        "stops"
      ],
      "metadata": {
        "id": "-yn_2yE9dLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleansing of list of words using lemmitizer and removing stop words"
      ],
      "metadata": {
        "id": "jlKxEE4zd07M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "FEouL1izbb7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def clean_review(words):\n",
        "  output_words =[]\n",
        "  for w in words:\n",
        "    if w.lower() not in stops:\n",
        "      pos = pos_tag([w])\n",
        "      clean_word = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n",
        "      output_words.append(clean_word.lower())\n",
        "  return output_words"
      ],
      "metadata": {
        "id": "_4yvu1ukmOsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docum = [(clean_review(words), category) for words, category in doc]"
      ],
      "metadata": {
        "id": "eGh5lmx9oPJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting dataset for training and testing "
      ],
      "metadata": {
        "id": "08ZWDTSvePkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(docum)\n"
      ],
      "metadata": {
        "id": "md3QEShx0-aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(docum)\n",
        "n"
      ],
      "metadata": {
        "id": "8n35Qpj3emDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(.75*n)"
      ],
      "metadata": {
        "id": "CbNqiTwHeuIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_documents = documents[0:1500]\n",
        "testing_documents = documents[1500:]"
      ],
      "metadata": {
        "id": "C0EABTCxHnpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "collecting unique words and separating out the top 5000 frequently used words"
      ],
      "metadata": {
        "id": "3P7so9r0fN4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for doc in training_documents:\n",
        "  all_words+=doc[0]"
      ],
      "metadata": {
        "id": "IbGiOZq0IC49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = nltk.FreqDist(all_words)\n",
        "common = freq.most_common(5000)\n",
        "features = [i[0] for i in common]\n"
      ],
      "metadata": {
        "id": "7P9AEQkOIeUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "mEK-FGyCJR8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making a feature dictionary for each review**"
      ],
      "metadata": {
        "id": "VuzTQHJeJp3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_dict(words):\n",
        "  current_features = {}\n",
        "  word_set = set(words)\n",
        "  for w in features:\n",
        "    current_features[w] = w in word_set\n",
        "  return current_features\n"
      ],
      "metadata": {
        "id": "YjvZbzGYJ2c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_feature_dict(training_documents[0][0])"
      ],
      "metadata": {
        "id": "B7S7WJV-K7qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training and Testing**"
      ],
      "metadata": {
        "id": "IA5LpqUvNTNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [(get_feature_dict(doc), category) for doc, category in training_documents]"
      ],
      "metadata": {
        "id": "ZZ75gEvMLI03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data = [(get_feature_dict(doc), category) for doc, category in testing_documents]"
      ],
      "metadata": {
        "id": "_AqD7mugLr6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(training_data)"
      ],
      "metadata": {
        "id": "GepDve6RMHoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.classify.accuracy(classifier,testing_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd1xExUGMWPK",
        "outputId": "593c4df9-a614-415c-e175-12fb7c95f60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.812"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOm4L2IDMtTf",
        "outputId": "7d879331-c91b-4f95-b9f3-221ac380e441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             outstanding = True              pos : neg    =     10.8 : 1.0\n",
            "             wonderfully = True              pos : neg    =      9.4 : 1.0\n",
            "                   anger = True              pos : neg    =      8.7 : 1.0\n",
            "               ludicrous = True              neg : pos    =      8.5 : 1.0\n",
            "                  welles = True              neg : pos    =      8.0 : 1.0\n",
            "               stupidity = True              neg : pos    =      7.9 : 1.0\n",
            "             magnificent = True              pos : neg    =      7.6 : 1.0\n",
            "                   inept = True              neg : pos    =      7.5 : 1.0\n",
            "                 stiller = True              pos : neg    =      7.4 : 1.0\n",
            "                  poorly = True              neg : pos    =      6.9 : 1.0\n",
            "                    lame = True              neg : pos    =      6.8 : 1.0\n",
            "                 idiotic = True              neg : pos    =      6.4 : 1.0\n",
            "                   damon = True              pos : neg    =      6.2 : 1.0\n",
            "            breathtaking = True              pos : neg    =      6.1 : 1.0\n",
            "                  alicia = True              neg : pos    =      6.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "classifier_sklearn = SklearnClassifier(rfc)"
      ],
      "metadata": {
        "id": "XfxW45ljbVLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_sklearn.train(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYbUPI3xcra_",
        "outputId": "b69e6bca-4e8e-4075-cd48-51506667bba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(RandomForestClassifier())>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.classify.accuracy(classifier_sklearn, testing_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bPz3sjIcy8m",
        "outputId": "619545b8-4c94-410a-bb92-e3e3b611f664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.804"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}